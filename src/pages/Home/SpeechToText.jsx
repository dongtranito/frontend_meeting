import React, { useState, useEffect, useRef } from "react";
import * as sdk from "microsoft-cognitiveservices-speech-sdk";
import BoardScript from "./BoardScript";
import { formatMilliseconds } from "../../utils/time";
import { useTimer } from "../../hooks/useTimer";
import SpeechControls from "./SpeechControls";
import Chat from "./Chat";
import { fetchWithAuth } from "../../utils/fetchWithAuth";
import { useParams, useNavigate } from "react-router-dom";

const SpeechToText = ({ onTranscriptProcessed }) => {
    const [status, setStatus] = useState("idle"); // "idle" | "recording" | "paused"
    const [currentText, setCurrentText] = useState("");
    const [transcript, setTranscript] = useState([]);
    const [speakerMap, setSpeakerMap] = useState({});
    const [summaryData, setSummaryData] = useState(null);
    const [bienbanData, setBienbanData] = useState("ch∆∞a c√≥ bi√™n b·∫£n c≈©");
    const [thoiGianNop, setThoiGianNop] = useState(null);
    const [meetingId, setMeetingId] = useState(null);

    const navigate = useNavigate();
    const { meetingID } = useParams();  // khi m√† kh√¥ng c√≥ c√°i meetingID n√†y th√¨ ta l·∫•y d·ªØ li·ªáu t·ª´ local. ng∆∞·ª£c l·∫°i th√¨ fetch d·ªØ li·ªáu
    // console.log(meetingID)

    const transcriberRef = useRef(null);
    const speakerMapRef = useRef({});
    const {
        elapsedTime,
        elapsedTimeRef,
        start: startTimer,
        pause: pauseTimer,
        reset: resetTimer,
    } = useTimer();

    // Load transcript t·ª´ localStorage khi v√†o trang
    useEffect(() => {

        if (meetingID) {
            fetchWithAuth(`http://localhost:3001/getMeetingDetail/${meetingID}`, {
                method: "GET"
            })
                .then(res => res.json())
                .then(data => {

                    // console.log(data)
                    onTranscriptProcessed(data);
                    setBienbanData(data.bienBanData);
                    setSummaryData(data.summaryData);
                    setMeetingId(data.meetingId);
                    setTranscript(data.transcript);     
                    // C·∫≠p nh·∫≠t d·ªØ li·ªáu v√†o localStorage (khi nh·∫≠n ƒë∆∞·ª£c sumaryData)
                    localStorage.setItem("transcriptRaw", JSON.stringify(data));

                });
        } else {
            const transcriptRaw = localStorage.getItem("transcriptRaw");
            if (transcriptRaw) {
                const parsed = JSON.parse(transcriptRaw);
                setTranscript(parsed.transcript);
                setThoiGianNop(parsed.thoiGianKetThuc);
                setBienbanData(parsed.bienBanData);
                setSummaryData(parsed.summaryData);
                setMeetingId(parsed.meetingId || null);
                onTranscriptProcessed({
                    transcriptRaw: parsed,
                    summaryData: parsed.summaryData,
                    bienBanData: parsed.bienBanData,
                });
            }
        }
    }, []);


    const updateSpeakerName = (oldName, newName) => {
        const newMap = { ...speakerMap, [oldName]: newName };
        const updatedTranscript = transcript.map((line) =>
            line.speaker === oldName ? { ...line, speaker: newName } : line
        );
        setTranscript(updatedTranscript);
        setSpeakerMap(newMap);
        speakerMapRef.current = newMap;
    };

    const startRecognition = async () => {
        navigate("/");
        setStatus("recording");
        setCurrentText("üé§ ƒêang nghe...");
        setTranscript([]);
        setBienbanData("ch∆∞a c√≥ bi√™n b·∫£n c≈©");
        setMeetingId(null);
        setSummaryData(null);    // c√≥ set l·∫°i tr·∫°ng th√°i c·ªßa c√°c bi·∫øn n√†y nh∆∞ng m√† m√†n h√¨nh kh√¥ng thay ƒë·ªïi l√† t·∫°i v√¨ kh√¥ng c√≥ truy·ªÅn tr·∫°ng th√°i ƒëi, nh∆∞ng m√† ƒë∆∞·ª£c c√°i l√† l√†m gi√° tr·ªã c·ªßa n√≥ thay ƒë·ªïi khi ta b·∫Øt ƒë·∫ßu ghi √¢m l·∫°i 
        startTimer();

        localStorage.removeItem("transcriptRaw");
        try {
            const res = await fetchWithAuth("http://localhost:3001/api/token",
            );
            const { token, region } = await res.json();
            const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
            speechConfig.speechRecognitionLanguage = "vi-VN";
            speechConfig.setProperty(
                sdk.PropertyId.SpeechServiceConnection_EnableSpeakerDiarization,
                "true"
            );
            speechConfig.setProperty(
                sdk.PropertyId.SpeechServiceResponse_DiarizeSpeakerSegments,
                "true"
            );

            const audioConfig = sdk.AudioConfig.fromDefaultMicrophoneInput();
            const transcriber = new sdk.ConversationTranscriber(speechConfig, audioConfig);
            transcriberRef.current = transcriber;

            transcriber.transcribing = (s, e) => {
                setCurrentText(e.result.text);
            };

            transcriber.transcribed = (s, e) => {
                if (e.result.reason === sdk.ResultReason.RecognizedSpeech) {
                    const parsed = JSON.parse(e.result.json);
                    const speakerId = parsed.SpeakerId || "Kh√¥ng r√µ ng∆∞·ªùi n√≥i";
                    const displayText = parsed.DisplayText || "";

                    if (
                        speakerId.toLowerCase() === "unknown" ||
                        displayText.trim().split(/\s+/).length < 5
                    ) return;

                    const speaker = speakerMapRef.current[speakerId] || speakerId;
                    const durationInMs = parsed.Duration / 10000;
                    const speechStartTime = elapsedTimeRef.current - durationInMs;
                    const time = formatMilliseconds(speechStartTime);

                    setTranscript((prev) => [
                        ...prev,
                        { speaker, text: displayText, time },
                    ]);
                    setCurrentText("");
                }
            };

            transcriber.canceled = (s, e) => {
                console.error("Canceled", e.errorDetails);
                setCurrentText("‚õî Nh·∫≠n di·ªán b·ªã h·ªßy");
                setStatus("idle");
                resetTimer();
            };

            transcriber.startTranscribingAsync(
                () => console.log("üéôÔ∏è B·∫Øt ƒë·∫ßu transcribe..."),
                (err) => {
                    console.error("L·ªói transcribe:", err);
                    setCurrentText("‚ùå Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu nh·∫≠n di·ªán");
                    setStatus("idle");
                    resetTimer();
                }
            );
        } catch (err) {
            console.error("Speech error:", err);
            setCurrentText("‚ùå L·ªói khi nh·∫≠n di·ªán");
            setStatus("idle");
            resetTimer();
        }
    };

    const pauseRecognition = () => {
        transcriberRef.current?.stopTranscribingAsync(
            () => {
                setStatus("paused");
                setCurrentText("‚è∏Ô∏è ƒê√£ t·∫°m d·ª´ng");
                pauseTimer();
            },
            (err) => console.error("L·ªói pause:", err)
        );
    };

    const resumeRecognition = () => {
        transcriberRef.current?.startTranscribingAsync(
            () => {
                setStatus("recording");
                setCurrentText("‚ñ∂Ô∏è ƒêang ti·∫øp t·ª•c...");
                startTimer();
            },
            (err) => {
                console.error("L·ªói resume:", err);
                setCurrentText("‚ùå Kh√¥ng th·ªÉ ti·∫øp t·ª•c.");
                resetTimer();
            }
        );
    };

    const stopRecognition = () => {
        transcriberRef.current?.stopTranscribingAsync(
            () => {
                transcriberRef.current = null;
                const thoiGian = new Date().toLocaleString("vi-VN");
                setThoiGianNop(thoiGian);
                setStatus("idle");
                setCurrentText("‚èπÔ∏è ƒê√£ d·ª´ng ho√†n to√†n");
                resetTimer();
                localStorage.setItem("transcriptRaw", JSON.stringify({
                    transcript,
                    thoiGianKetThuc: thoiGianNop,
                }));
            },
            (err) => console.error("L·ªói stop:", err)
        );
    };


    // ·ªü h√†m n√†y th√¨ m√¨nh c√≥ th·ªÉ g·∫Øn bi·∫øn tr·∫°ng th√°i ƒë√£ g·ªüi, ho·∫∑c ƒë√£ g·ªüi xong, xong r√¥i r·ªìi truy·ªÅn xu·ªëng component con c≈©ng ƒë∆∞·ª£c, m√† m√¨nh l·ª° l√†m ·ªü component con r·ªìi th√¨ th√¥i
    const handleSubmitTranscript = (transcriptChat) => {
        if (transcript.length === 0) {
            alert("Ch∆∞a c√≥ n·ªôi dung ƒë·ªÉ g·ª≠i.");
            return;
        }

        return fetchWithAuth("http://localhost:3001/submitTranscript", {  // c√°i n√†y l√† m√¨nh return v·ªÅ m·ªôt promise, n·∫øu mu·ªën d·ªÖ nh√¨n th√¨ vi·∫øt b·∫±ng async 
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
                transcript,
                transcriptChat,
                thoiGianKetThuc: thoiGianNop,
                bienBanData: bienbanData,
                summaryData: summaryData,
                meetingId: meetingId,
            }),
            credentials: "include"
        })
            .then((res) => {
                if (!res.ok) throw new Error("HTTP error " + res.status);
                return res.json();
            })
            .then((data) => {
                onTranscriptProcessed(data);
                setBienbanData(data.bienBanData);
                setSummaryData(data.summaryData);
                setMeetingId(data.meetingId);
                // C·∫≠p nh·∫≠t d·ªØ li·ªáu v√†o localStorage (khi nh·∫≠n ƒë∆∞·ª£c sumaryData)
                localStorage.setItem("transcriptRaw", JSON.stringify(data));
            })
            .catch((err) => {
                console.error("L·ªói g·ª≠i:", err);
            });
    };

    return (
        <div className="w-full mx-auto py-4 px-6 bg-white rounded text-center relative h-full">
            {status === "idle" && transcript.length > 0 && (
                <div className="absolute bottom-4 w-full left-0">
                    <Chat onClick={handleSubmitTranscript} />
                </div>
            )}

            <SpeechControls
                status={status}
                onStart={startRecognition}
                onPause={pauseRecognition}
                onResume={resumeRecognition}
                onStop={stopRecognition}
                time={formatMilliseconds(elapsedTimeRef.current)}
            />

            <BoardScript
                scripts={transcript}
                currentText={currentText}
                onRenameSpeaker={updateSpeakerName}
            />
        </div>
    );
};

export default SpeechToText;
